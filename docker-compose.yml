# Production Docker Compose Configuration for Superego MCP Server
# Comprehensive multi-service setup with monitoring, logging, and security
# Based on Python MCP Docker Guide patterns

services:
  # Main Superego MCP Server
  superego-mcp:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
      args:
        BUILDKIT_INLINE_CACHE: 1
    image: superego-mcp:latest
    container_name: superego-mcp-prod
    restart: unless-stopped
    
    # Port mapping (internal only, exposed via reverse proxy)
    expose:
      - "8000"
    
    # Production environment configuration
    environment:
      # Core server configuration
      - SUPEREGO_HOST=0.0.0.0
      - SUPEREGO_PORT=8000
      - SUPEREGO_LOG_LEVEL=${SUPEREGO_LOG_LEVEL:-info}
      - SUPEREGO_CONFIG_PATH=/app/config/server.yaml
      
      # Security and performance
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONHASHSEED=0
      - PYTHONFAULTHANDLER=1
      
      # Production optimizations
      - SUPEREGO_ENV=production
      - SUPEREGO_DEBUG=${SUPEREGO_DEBUG:-false}
      - SUPEREGO_HOT_RELOAD=false
      
      # Monitoring and observability
      - SUPEREGO_METRICS_ENABLED=${SUPEREGO_METRICS_ENABLED:-true}
      - SUPEREGO_METRICS_PATH=/metrics
      - SUPEREGO_HEALTH_CHECK_PATH=/health
      - SUPEREGO_TRACING_ENABLED=${SUPEREGO_TRACING_ENABLED:-false}
      
      # AI service configuration
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - SUPEREGO_AI_PROVIDER=${SUPEREGO_AI_PROVIDER:-anthropic}
      - SUPEREGO_AI_MODEL=${SUPEREGO_AI_MODEL:-claude-3-haiku-20240307}
      - SUPEREGO_AI_TIMEOUT=${SUPEREGO_AI_TIMEOUT:-30}
      - SUPEREGO_AI_MAX_RETRIES=${SUPEREGO_AI_MAX_RETRIES:-3}
      
      # Claude CLI inference provider (supports both auth methods)
      - CLAUDE_CODE_OAUTH_TOKEN=${CLAUDE_CODE_OAUTH_TOKEN}  # OAuth token authentication
      
      # Performance settings
      - SUPEREGO_WORKERS=${SUPEREGO_WORKERS:-1}
      - SUPEREGO_MAX_REQUESTS=${SUPEREGO_MAX_REQUESTS:-1000}
      - SUPEREGO_REQUEST_TIMEOUT=${SUPEREGO_REQUEST_TIMEOUT:-60}
      - SUPEREGO_KEEPALIVE_TIMEOUT=${SUPEREGO_KEEPALIVE_TIMEOUT:-5}
      
      # Security configuration
      - SUPEREGO_CORS_ORIGINS=${SUPEREGO_CORS_ORIGINS:-}
      - SUPEREGO_API_KEY=${SUPEREGO_API_KEY:-}
      - SUPEREGO_RATE_LIMIT_ENABLED=${SUPEREGO_RATE_LIMIT_ENABLED:-true}
      - SUPEREGO_RATE_LIMIT_REQUESTS=${SUPEREGO_RATE_LIMIT_REQUESTS:-100}
      - SUPEREGO_RATE_LIMIT_WINDOW=${SUPEREGO_RATE_LIMIT_WINDOW:-60}
      
      # Explicit logging configuration for production Docker
      - SUPEREGO_LOG_FORMAT=json     # JSON structured logs for production
      - SUPEREGO_LOG_HANDLER=write   # WriteLogger for Docker containers (prevents I/O errors)
    
    # Volume mounts for persistent data and configuration
    volumes:
      # Configuration directory - unified with development
      - ./config:/app/config:ro
      - superego-logs:/app/logs
      
      # Optional: Custom TLS certificates
      - ${SUPEREGO_TLS_CERT:-/dev/null}:/app/certs/server.crt:ro
      - ${SUPEREGO_TLS_KEY:-/dev/null}:/app/certs/server.key:ro
    
    # Health check configuration
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource limits for production
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service,environment"
    
    # Labels for service discovery and monitoring
    labels:
      - "service=superego-mcp"
      - "environment=production"
      - "version=${SUPEREGO_VERSION:-latest}"
      - "component=mcp-server"
      - "traefik.enable=true"
      - "traefik.http.routers.superego.rule=Host(`${SUPEREGO_DOMAIN:-superego.local}`)"
      - "traefik.http.services.superego.loadbalancer.server.port=8000"
    
    # Network configuration
    networks:
      - superego-prod
      - monitoring
    
    depends_on:
      redis:
        condition: service_healthy

  # Redis for caching and session management
  redis:
    image: redis:7-alpine
    container_name: superego-redis
    restart: unless-stopped
    
    # Redis configuration
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    
    # Volume for Redis persistence
    volumes:
      - redis-data:/data
      - ${REDIS_CONFIG_FILE:-/dev/null}:/usr/local/etc/redis/redis.conf:ro
    
    # Health check
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    
    # Security: no external ports exposed
    expose:
      - "6379"
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    
    networks:
      - superego-prod

  # Nginx reverse proxy for production deployment
  nginx:
    image: nginx:1.25-alpine
    container_name: superego-nginx
    restart: unless-stopped
    
    # External port mapping
    ports:
      - "${SUPEREGO_HTTP_PORT:-80}:80"
      - "${SUPEREGO_HTTPS_PORT:-443}:443"
    
    # Nginx configuration
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
    
    # Environment for configuration templating
    environment:
      - SUPEREGO_UPSTREAM=superego-mcp:8000
      - SUPEREGO_DOMAIN=${SUPEREGO_DOMAIN:-superego.local}
      - NGINX_WORKER_PROCESSES=${NGINX_WORKER_PROCESSES:-auto}
      - NGINX_WORKER_CONNECTIONS=${NGINX_WORKER_CONNECTIONS:-1024}
    
    # Health check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    
    labels:
      - "service=nginx"
      - "environment=production"
      - "component=reverse-proxy"
    
    networks:
      - superego-prod
    
    depends_on:
      superego-mcp:
        condition: service_healthy

  # Prometheus monitoring
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: superego-prometheus
    restart: unless-stopped
    
    # Prometheus configuration
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.route-prefix=/'
      - '--web.external-url=http://${SUPEREGO_DOMAIN:-localhost}/prometheus'
    
    # Configuration and data volumes
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    
    # Internal port only
    expose:
      - "9090"
    
    # Health check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    
    labels:
      - "service=prometheus"
      - "environment=production"
      - "component=monitoring"
    
    networks:
      - monitoring
    
    profiles: ["monitoring", "full"]

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.2.0
    container_name: superego-grafana
    restart: unless-stopped
    
    # Environment configuration
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_DOMAIN=${SUPEREGO_DOMAIN:-localhost}
      - GF_SERVER_ROOT_URL=http://${SUPEREGO_DOMAIN:-localhost}/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_DATABASE_TYPE=sqlite3
      - GF_DATABASE_PATH=/var/lib/grafana/grafana.db
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=${GRAFANA_PLUGINS:-}
    
    # Data and configuration volumes
    volumes:
      - grafana-data:/var/lib/grafana
      - grafana-logs:/var/log/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    
    # Internal port only
    expose:
      - "3000"
    
    # Health check
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    
    labels:
      - "service=grafana"
      - "environment=production"
      - "component=monitoring"
    
    networks:
      - monitoring
    
    depends_on:
      prometheus:
        condition: service_healthy
    
    profiles: ["monitoring", "full"]

  # Log aggregation with Loki
  loki:
    image: grafana/loki:2.9.0
    container_name: superego-loki
    restart: unless-stopped
    
    # Loki configuration
    command: -config.file=/etc/loki/local-config.yaml
    
    volumes:
      - ./monitoring/loki.yml:/etc/loki/local-config.yaml:ro
      - loki-data:/tmp/loki
    
    expose:
      - "3100"
    
    # Health check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    
    networks:
      - monitoring
    
    profiles: ["logging", "full"]

  # Promtail for log collection
  promtail:
    image: grafana/promtail:2.9.0
    container_name: superego-promtail
    restart: unless-stopped
    
    # Promtail configuration
    command: -config.file=/etc/promtail/config.yml
    
    volumes:
      - ./monitoring/promtail.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    # Health check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M
    
    networks:
      - monitoring
    
    depends_on:
      loki:
        condition: service_healthy
    
    profiles: ["logging", "full"]

# Named volumes for persistent data
volumes:
  # Application data
  superego-config:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/config
      o: bind
  superego-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/data/logs
      o: bind
  
  # Database and cache
  redis-data:
    driver: local
  
  # Reverse proxy
  nginx-cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=100m
  nginx-logs:
    driver: local
  
  # Monitoring
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  grafana-logs:
    driver: local
  loki-data:
    driver: local

# Networks for service isolation
networks:
  # Main application network
  superego-prod:
    driver: bridge
    name: superego-prod-network
    ipam:
      config:
        - subnet: 172.20.0.0/24
    driver_opts:
      com.docker.network.bridge.name: superego-prod0
  
  # Monitoring network
  monitoring:
    driver: bridge
    name: superego-monitoring-network
    ipam:
      config:
        - subnet: 172.21.0.0/24
    driver_opts:
      com.docker.network.bridge.name: superego-mon0

# Profiles for different deployment scenarios
# Usage:
#   docker-compose up                              # Basic: superego-mcp + redis + nginx
#   docker-compose --profile monitoring up        # Add Prometheus + Grafana
#   docker-compose --profile logging up           # Add Loki + Promtail
#   docker-compose --profile full up              # Complete stack
x-profiles:
  basic: &basic-services
    - superego-mcp
    - redis
    - nginx
  monitoring: &monitoring-services
    - prometheus
    - grafana
  logging: &logging-services
    - loki
    - promtail
  full: &full-services
    - superego-mcp
    - redis
    - nginx
    - prometheus
    - grafana
    - loki
    - promtail